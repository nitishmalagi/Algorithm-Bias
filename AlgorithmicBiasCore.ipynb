{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Bias - Core Code\n",
    "Some code to get started on the Algorithmic Bias assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "bcDB = datasets.load_breast_cancer()\n",
    "bcDF = pd.DataFrame(bcDB.data, columns= list(bcDB['feature_names']))\n",
    "bcDF['target'] = pd.Series(bcDB.target)\n",
    "bcDF = bcDF.sort_values(by = ['target'])\n",
    "bcDF = bcDF.reset_index(drop=True)\n",
    "bcDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vc = bcDF['target'].value_counts()\n",
    "for i,j in enumerate(bcDB.target_names):\n",
    "    print (vc[i],j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = bcDF.pop('target').values\n",
    "X = bcDF.values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-NN\n",
    "#### Malignant is the minority class at ~40%.  \n",
    "#### $k$-NN classifier picks up this under-representation and accentuates it, predicting just 36% malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=2)\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "#bias?\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy hold-out: {0:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_knn)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_knn, classes=['Not Helpful','Helpful'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "#### Decision Tree classifier picks up this under-representation of malignant but does a pretty good job of predicting 39% malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "ap_features = bcDF.columns\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "bc_tree = tree.fit(X_train, y_train)\n",
    "y_pred_tree = bc_tree.predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_tree.sum()/len(y_pred_tree))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_tree)\n",
    "print(\"Accuracy hold-out: {0:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_tree)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_tree, classes=['Not Helpful','Helpful'], normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "#### Logistic Regression classifier picks up this under-representation of malignant but predicts just 36% malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_lr.sum()/len(y_pred_lr))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Accuracy hold-out: {0:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_lr, classes=['Not Helpful','Helpful'], normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes\n",
    "\n",
    "#### Naives Bayes classifier picks up this under-representation of malignant and predicts just 37% malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_gnb.sum()/len(y_pred_gnb))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_gnb)\n",
    "print(\"Accuracy hold-out: {0:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_gnb)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_gnb, classes=['Not Helpful','Helpful'], normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Check using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
    "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    "\n",
    "models = [kNN, bc_tree, log_reg, gnb]\n",
    "\n",
    "folds = 4\n",
    "v = 0 #  use 1 or 0\n",
    "\n",
    "for m in models:\n",
    "    cv_results = cross_validate(m, X, y, cv= folds,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = v, n_jobs = -1)\n",
    "    fp_rate = cv_results['test_fp'].sum()/(cv_results['test_fp'].sum()+cv_results['test_tn'].sum())\n",
    "    tp_rate = cv_results['test_tp'].sum()/(cv_results['test_tp'].sum()+cv_results['test_fn'].sum())\n",
    "  \n",
    "    print(\"{} x CV {:22} FP: {:.2f}  TP: {:.2f}\".format(folds, type(m).__name__, fp_rate, tp_rate)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation testing results are as follows:\n",
    "##### KNN : 0.96,  Decision Tree : 0.94,  Logistic Regression : 0.97,  GaussianNB : 0.97\n",
    "#### False positive denotes the bias of the model towards Benign from the above results we can see that GaussianNB and Logistic Regression have the highest True Positives at 97%. However, since Logistic Regression predicts the least False Positives, it is clearly a better model than the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Propose a strategy to rectify this bias. Evaluate the effect of this strategy in terms of classification bias and overall accuracy. You may choose to work with hold-out testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The bias can be varified in various methods. Here, I have used a couple of methods which are upsampling and downlsampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "bcDB = datasets.load_breast_cancer()\n",
    "bcDF = pd.DataFrame(bcDB.data, columns= list(bcDB['feature_names']))\n",
    "bcDF['target'] = pd.Series(bcDB.target)\n",
    "bcDF = bcDF.sort_values(by = ['target'])\n",
    "bcDF = bcDF.reset_index(drop=True)\n",
    "y = bcDF.target\n",
    "X = bcDF.drop('target', axis=1)\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "mel = X[X.target==1]\n",
    "beg = X[X.target==0]\n",
    "\n",
    "\n",
    "print('**** Upsampling ****')\n",
    "\n",
    "# upsample minority\n",
    "beg_upsampled = resample(beg,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(mel), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([mel, beg_upsampled])\n",
    "\n",
    "y_train = upsampled.target\n",
    "X_train = upsampled.drop('target', axis=1)\n",
    "\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Upsampling Accuracy hold-out: {:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_knn)\n",
    "print(\"Upsampling Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "\n",
    "\n",
    "print('**** Downsampling ****')\n",
    "\n",
    "#downsample majority\n",
    "mel_downsampled = resample(mel,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(beg), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "downsampled = pd.concat([beg, mel_downsampled])\n",
    "\n",
    "y_train = downsampled.target\n",
    "X_train = downsampled.drop('target', axis=1)\n",
    "\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Downsampling Accuracy hold-out: {:.2f}\".format(acc)) \n",
    "confusion = confusion_matrix(y_test, y_pred_knn)\n",
    "print(\"Downsampling Confusion matrix:\\n{}\".format(confusion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this scenario, after upsampling and downsampling we are getting similar results.The accuracy increases from 91% to 92%. However, there is there is a small increase in the True-Positive values. The bias is more towards the majortiy class when compared to that of the imbalanced dataset. In conclusion, there is no much profit from this type of method to balance the dataset as the results could vary depending on the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test the impact of this strategy on another dataset of your choice. Discuss the effectiveness of the strategy on this second dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset considered here is Wine dataset with two classes 0 and 1.\n",
    "\n",
    "#### In this imbalanced dataset, Class 0 has 43 observations and Class 1 has 61 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "wineDF = pd.read_csv('wine.csv')\n",
    "wineDF = wineDF.reset_index(drop=True)\n",
    "wineDF.head()\n",
    "\n",
    "y = wineDF['class']\n",
    "X = wineDF.drop('class', axis=1)\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(\"**** With imbalanced dataset ****\")\n",
    "\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy hold-out: {0:.2f}\".format(acc)) \n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_knn, classes=['Not Helpful','Helpful'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "type1 = X[X['class'] == 0]\n",
    "type2 = X[X['class'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('**** Upsampling ****')\n",
    "# upsample minority\n",
    "type1_upsampled = resample(type1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(type2), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([type2, type1_upsampled])\n",
    "\n",
    "\n",
    "y_train = upsampled['class']\n",
    "X_train = upsampled.drop('class', axis=1)\n",
    "\n",
    "\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Upsampling Accuracy hold-out: {:.2f}\".format(acc)) \n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_knn, classes=['Not Helpful','Helpful'],\n",
    "                      title='Confusion matrix, without normalization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('**** Downsampling ****')\n",
    "#downsample majority\n",
    "type2_downsampled = resample(type2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(type1), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "# combine majority and upsampled minority\n",
    "downsampled = pd.concat([type1, type2_downsampled])\n",
    "\n",
    "\n",
    "y_train = downsampled['class']\n",
    "X_train = downsampled.drop('class', axis=1)\n",
    "\n",
    "\n",
    "y_pred_knn = kNN.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred_knn.sum()/len(y_pred_knn))))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Upsampling Accuracy hold-out: {:.2f}\".format(acc)) \n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_knn, classes=['Not Helpful','Helpful'],\n",
    "                      title='Confusion matrix, without normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy on testing with imbalanced dataset is 85%.\n",
    "\n",
    "#### We are handling this imbalance with upsampling and downsampling. The accuracy of the model after upsampling is 92%. Moreover, it is interesting to note that the accuarcy shoots to 100% with downsampling.\n",
    "\n",
    "#### However, it is not always the case because downsampling leads to removal of useful information which can effect the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
